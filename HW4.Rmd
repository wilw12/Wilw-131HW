---
title: "HW4"
output: html_document
date: "2022-11-07"
---

```{r include=FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
tidymodels_prefer()
```
## Question 1
```{r split data}
df <- read.csv('/Users/williamwang/Desktop/Wilw-131HW/titanic.csv') %>%
  mutate(survived = factor(survived, 
                           levels = c("Yes", "No")),
         pclass = factor(pclass))
set.seed(891) # keep the outcome stable
titanic_split <- initial_split(df, prop = 0.80, strata = survived) # split the data and stratified on survived
titanic_training <- training(titanic_split) # extract the training data
titanic_testing <- testing(titanic_split) # extract the testing data
```

## Question 2
```{r k-fold CV}
titanic_fold <- vfold_cv(titanic_training, v=10)
titanic_fold
```

## Question 3
The data set is split into 10 folds, and it can be used to evaluate the performance of model when given new data.
When using k-fold cross-validation, all parts of data will be used as parts of testing data, which is helpful to evaluate the performance of model. 
If we want to use the entire data set, we can use leave-one-out cross validation. 

## Question 4
```{r dummy recipe}
titanic_recipe <- recipe(survived ~ pclass+sex+age+sib_sp+parch+fare, data = titanic_training) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ starts_with('sex'):fare+
                  age:fare)
summary(titanic_recipe)
```
```{r engine & workflow}
log_reg <- logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')
log_wf <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
#log_fit <- fit(log_wf, titanic_training)
```
```{r LDA}
lda_mode <- discrim_linear() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')
lda_wf <- workflow() %>% 
  add_model(lda_mode) %>% 
  add_recipe(titanic_recipe)
#lda_fit <- fit(lda_wf, titanic_training)
```
```{r QDA}
qda_mode <- discrim_quad() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')
qda_wf <- workflow() %>% 
  add_model(qda_mode) %>% 
  add_recipe(titanic_recipe)
#qda_fit <- fit(qda_wf, titanic_training)
```

## Question 5
```{r logistic fit}
log_fit <- fit_resamples(log_wf, titanic_fold)
```
```{r lda}
lda_fit <- fit_resamples(lda_wf, titanic_fold)
```
```{r qda}
qda_fit <- fit_resamples(qda_wf, titanic_fold)
```

## Question 6
```{r}
mx1 <- collect_metrics(log_fit)
mx1
mx2 <- collect_metrics(lda_fit)
mx2
mx3 <- collect_metrics(qda_fit)
mx3
results <- bind_rows(mx1, mx2, mx3) %>%
  tibble() %>% mutate(model = rep(c("Logistic Regression", "Linear Discrminant Analysis", "Quadratic Discrminant Analysis"), each = 2)) %>%
  dplyr::select(model, .metric, mean, std_err)
results
```
logistic regression seems to be the best model. 

## Question 7
```{r}
log_new_fit <- fit(log_wf, titanic_training)
```

## Question 8
```{r}
log_acc <- predict(log_new_fit, new_data = titanic_testing, type = 'class') %>% 
  bind_cols(titanic_testing %>% select(survived)) %>% 
  accuracy(truth = survived, estimate = .pred_class)
log_acc
```